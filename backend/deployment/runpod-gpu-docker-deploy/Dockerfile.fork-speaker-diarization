# =============================================================================
# Meetily RunPod GPU Services - Speaker Diarization (Pre-built)
# Whisper.cpp + Pyannote Speaker Diarization + Ollama LLM
# Builds everything at image build time for faster pod startup
# =============================================================================
FROM runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

LABEL maintainer="Meetily"
LABEL description="GPU-accelerated Whisper + Speaker Diarization + Ollama (pre-built)"

ENV DEBIAN_FRONTEND=noninteractive

# -----------------------------------------------------------------------------
# 1. Install System Dependencies
# -----------------------------------------------------------------------------
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    ffmpeg \
    pkg-config \
    libsdl2-dev \
    zstd \
    && rm -rf /var/lib/apt/lists/*

# -----------------------------------------------------------------------------
# 2. Install Python dependencies for diarization service
# -----------------------------------------------------------------------------
RUN pip install --no-cache-dir \
    fastapi>=0.115.0 \
    "uvicorn[standard]>=0.34.0" \
    python-multipart>=0.0.9 \
    httpx>=0.27.0 \
    "torch<2.6" \
    "torchaudio<2.6" \
    "huggingface_hub>=0.20.0,<0.23.0" \
    "pyannote.audio>=3.1.0,<4.0"

# -----------------------------------------------------------------------------
# 3. Clone Repository and Build Whisper Server
# -----------------------------------------------------------------------------
WORKDIR /workspace

RUN git clone -b feat/speaker-diarization https://github.com/kimwwk/meeting-minutes.git && \
    cd meeting-minutes && \
    git submodule update --init --recursive

WORKDIR /workspace/meeting-minutes/backend/whisper.cpp

RUN cmake -B build \
    -DCMAKE_BUILD_TYPE=Release \
    -DWHISPER_BUILD_SERVER=ON \
    -DWHISPER_BUILD_EXAMPLES=ON \
    -DWHISPER_BUILD_TESTS=OFF \
    -DBUILD_SHARED_LIBS=OFF \
    -DGGML_STATIC=ON \
    -DGGML_CUDA=ON \
    -DGGML_NATIVE=OFF && \
    cmake --build build --config Release --target whisper-server -j$(nproc)

# -----------------------------------------------------------------------------
# 4. Download Whisper Model
# -----------------------------------------------------------------------------
ARG WHISPER_MODEL=base.en
ENV WHISPER_MODEL=${WHISPER_MODEL}

RUN mkdir -p /workspace/meeting-minutes/backend/models && \
    wget -q --show-progress -O "/workspace/meeting-minutes/backend/models/ggml-${WHISPER_MODEL}.bin" \
    "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-${WHISPER_MODEL}.bin"

# -----------------------------------------------------------------------------
# 5. Install Ollama
# -----------------------------------------------------------------------------
RUN curl -fsSL https://ollama.com/install.sh | sh

# -----------------------------------------------------------------------------
# 6. Create Start Script
# -----------------------------------------------------------------------------
RUN mkdir -p /opt/meetily

RUN cat > /opt/meetily/start-gpu-services.sh << 'START_EOF'
#!/bin/bash

echo "Starting Meetily GPU Services with Diarization + Ollama..."

# Re-install Ollama if missing (RunPod clears /usr on restart)
if ! command -v ollama &> /dev/null; then
    echo "Ollama not found. Reinstalling..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# Set environment
export WHISPER_MODEL=${WHISPER_MODEL:-base.en}
export OLLAMA_HOST=0.0.0.0:11434
export HF_AUTH_TOKEN=${HF_AUTH_TOKEN:-}

# Start Ollama server
echo "Starting Ollama server on port 11434..."
ollama serve &
sleep 5

# Wait for Ollama to be ready
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "Ollama server is ready"
        break
    fi
    sleep 1
done

# Pull default model if needed
ollama pull llama3.2:3b 2>/dev/null || echo "llama3.2:3b already available"

# Start Whisper server
echo "Starting Whisper server on port 8178..."
cd /workspace/meeting-minutes/backend/whisper.cpp
./build/bin/whisper-server \
    --model /workspace/meeting-minutes/backend/models/ggml-${WHISPER_MODEL}.bin \
    --host 0.0.0.0 \
    --port 8178 \
    --threads 4 &

# Wait for Whisper server
for i in {1..30}; do
    if curl -s http://localhost:8178/ > /dev/null 2>&1; then
        echo "Whisper server is ready"
        break
    fi
    sleep 1
done

# Start Diarization service
echo "Starting Diarization service on port 8179..."
cd /workspace/meeting-minutes/backend
python3 -m uvicorn diarization_service.main:app \
    --host 0.0.0.0 \
    --port 8179 &

echo ""
echo "==========================================="
echo "  Meetily GPU Services Started!"
echo "==========================================="
echo "  Ollama:       http://0.0.0.0:11434"
echo "  Whisper:      http://0.0.0.0:8178"
echo "  Diarization:  http://0.0.0.0:8179"
echo "==========================================="

wait
START_EOF

RUN cat > /opt/meetily/test-gpu-services.sh << 'TEST_EOF'
#!/bin/bash
echo "Testing services..."
curl -s http://localhost:11434/api/tags > /dev/null && echo "Ollama: OK" || echo "Ollama: FAILED"
curl -s http://localhost:8178/ > /dev/null && echo "Whisper: OK" || echo "Whisper: FAILED"
curl -s http://localhost:8179/health > /dev/null && echo "Diarization: OK" || echo "Diarization: FAILED"
TEST_EOF

RUN chmod +x /opt/meetily/*.sh

# -----------------------------------------------------------------------------
# 7. Expose Ports
# -----------------------------------------------------------------------------
EXPOSE 8178 8179 11434

CMD ["/opt/meetily/start-gpu-services.sh"]
